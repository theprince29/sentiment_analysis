{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from  nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target             0\n",
      "id                 0\n",
      "date               0\n",
      "flag               0\n",
      "user               0\n",
      "text               0\n",
      "stemmed_content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "twitter_data = pd.read_csv('twitter_data_stemmed.csv')\n",
    "twitter_data['stemmed_content'].replace({np.nan: ''}, inplace=True)\n",
    "nan_counts = twitter_data.isna().sum()\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stemmed_content    1600000\n",
       "target             1600000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit = twitter_data[[\"stemmed_content\", \"target\"]]\n",
    "twit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>head hurt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meet ellen lunch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtv movi award tonit hp sneak peek fucck chyeahh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chantiparnel awww bless</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>welshstev thank bro think late night dubai cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>batteri die x fairrrrrrr wanna play x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>followfriday nickcharney give faith hope hell ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>david henri http twitpic com fumn awwwww preci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>man arm hurt like hell get vaccin start colleg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>plan get simpli springfield websit run next we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           stemmed_content  target\n",
       "0                                                head hurt       0\n",
       "1                                         meet ellen lunch       1\n",
       "2         mtv movi award tonit hp sneak peek fucck chyeahh       1\n",
       "3                                  chantiparnel awww bless       0\n",
       "4        welshstev thank bro think late night dubai cha...       1\n",
       "...                                                    ...     ...\n",
       "1599995              batteri die x fairrrrrrr wanna play x       0\n",
       "1599996  followfriday nickcharney give faith hope hell ...       1\n",
       "1599997  david henri http twitpic com fumn awwwww preci...       1\n",
       "1599998     man arm hurt like hell get vaccin start colleg       0\n",
       "1599999  plan get simpli springfield websit run next we...       1\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 2)\n",
      "(800000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stemmed_content    1600000\n",
       "target             1600000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_array = twit[twit[\"target\"] == 1].reset_index(drop=True)\n",
    "negative_array = twit[twit[\"target\"] == 0].reset_index(drop=True)\n",
    "print(positive_array.shape)\n",
    "print(negative_array.shape)\n",
    "#  Concatenate the two subsets to create the final DataFrame with equal counts of both classes\n",
    "new_df = pd.concat([positive_array, negative_array])\n",
    "\n",
    "# Shuffle the rows to randomize the order\n",
    "new_df = new_df.sample(frac=1).reset_index(drop=True)\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training set shape: (1280000, 2)\n",
      "Balanced testing set shape: (320000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate positive and negative samples\n",
    "positive_samples = new_df[new_df['target'] == 1]\n",
    "negative_samples = new_df[new_df['target'] == 0]\n",
    "\n",
    "# Split positive samples into training and testing sets\n",
    "positive_train, positive_test = train_test_split(positive_samples, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split negative samples into training and testing sets\n",
    "negative_train, negative_test = train_test_split(negative_samples, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine positive and negative training sets to create balanced training set\n",
    "balanced_train = pd.concat([positive_train, negative_train])\n",
    "\n",
    "# Combine positive and negative testing sets to create balanced testing set\n",
    "balanced_test = pd.concat([positive_test, negative_test])\n",
    "\n",
    "# Shuffle the rows to randomize the order\n",
    "balanced_train = balanced_train.sample(frac=1).reset_index(drop=True)\n",
    "balanced_test = balanced_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shapes of the balanced training and testing sets\n",
    "print(\"Balanced training set shape:\", balanced_train.shape)\n",
    "print(\"Balanced testing set shape:\", balanced_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          stemmed_content  target\n",
      "0                            soozenw methink poo come way       1\n",
      "1                                destinysport anyth angel       1\n",
      "2                            realli want make pooram year       0\n",
      "3       love brother birthday sang ridicul song voicem...       1\n",
      "4       secretbeav may heal comfort quick possibl may ...       1\n",
      "...                                                   ...     ...\n",
      "319995  hey teamdemi lovato awww got go say bye hehe t...       0\n",
      "319996                                   massiv morn stuf       1\n",
      "319997                                    think conan hot       1\n",
      "319998                 go work black drunk time juci hate       0\n",
      "319999                             kamsmommi naw nevr got       0\n",
      "\n",
      "[320000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(balanced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative tweets\n",
    "positive_tweets = new_df[new_df['target'] == 1]\n",
    "negative_tweets = new_df[new_df['target'] == 0]\n",
    "\n",
    "# Function to generate and plot word clouds\n",
    "def generate_word_cloud(data, sentiment):\n",
    "    all_text = ' '.join(data['stemmed_content'])\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "    # Plot word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {sentiment} Tweets')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               positive_freq  neg_freq  total_freq\n",
      "digita                     0         1           1\n",
      "smileychub                 1         0           1\n",
      "tualp                      0         1           1\n",
      "andrevr                    7         8          15\n",
      "prodhack                   1         0           1\n",
      "...                      ...       ...         ...\n",
      "sshayler                   1         0           1\n",
      "pisd                       0         4           4\n",
      "extant                     1         0           1\n",
      "lirraangelica              0         1           1\n",
      "ahj                       10        17          27\n",
      "\n",
      "[461054 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_frequency_df(new_df):\n",
    "    # Initialize frequency dictionaries\n",
    "    positive_freq_dict = defaultdict(int)\n",
    "    negative_freq_dict = defaultdict(int)\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in new_df.iterrows():\n",
    "        # Tokenize the stemmed content into individual words\n",
    "        words = row['stemmed_content'].split()\n",
    "        # Update frequency dictionaries based on target class\n",
    "        for word in words:\n",
    "            if row['target'] == 1:\n",
    "                positive_freq_dict[word] += 1\n",
    "            else:\n",
    "                negative_freq_dict[word] += 1\n",
    "\n",
    "    # Combine positive and negative frequency dictionaries to get unique words\n",
    "    unique_words = set(positive_freq_dict.keys()).union(negative_freq_dict.keys())\n",
    "\n",
    "    # Create a DataFrame from frequency dictionaries\n",
    "    frequency_df = pd.DataFrame({\n",
    "        'positive_freq': [positive_freq_dict[word] for word in unique_words],\n",
    "        'neg_freq': [negative_freq_dict[word] for word in unique_words]\n",
    "    },index = list(unique_words))\n",
    "    frequency_df['total_freq'] = frequency_df['positive_freq'] + frequency_df['neg_freq']\n",
    "    return frequency_df\n",
    "\n",
    "\n",
    "\n",
    "# Create the frequency DataFrame\n",
    "frequency_df = create_frequency_df(balanced_train)\n",
    "\n",
    "# Display the frequency DataFrame\n",
    "print(frequency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_positive_sum = frequency_df['positive_freq'].sum()\n",
    "total_negative_sum = frequency_df['neg_freq'].sum()\n",
    "\n",
    "v = len(frequency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_df['w|pos'] = (frequency_df['positive_freq'] + 1)/(total_positive_sum + v)\n",
    "frequency_df['w|neg'] = (frequency_df['neg_freq'] + 1)/(total_negative_sum + v)\n",
    "\n",
    "frequency_df['lamda'] = np.log(frequency_df['w|pos']/frequency_df['w|neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(frequency_df['w|pos'].sum())\n",
    "print(frequency_df['w|neg'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive_freq  neg_freq  total_freq         w|pos         w|neg  \\\n",
      "digita                  0         1           1  1.842891e-07  3.711613e-07   \n",
      "smileychub              1         0           1  3.685782e-07  1.855806e-07   \n",
      "tualp                   0         1           1  1.842891e-07  3.711613e-07   \n",
      "andrevr                 7         8          15  1.474313e-06  1.670226e-06   \n",
      "prodhack                1         0           1  3.685782e-07  1.855806e-07   \n",
      "\n",
      "               lamda  \n",
      "digita     -0.700131  \n",
      "smileychub  0.686163  \n",
      "tualp      -0.700131  \n",
      "andrevr    -0.124767  \n",
      "prodhack    0.686163  \n"
     ]
    }
   ],
   "source": [
    "print(frequency_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set: 76.185625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate accuracy of a prediction function on testing data\n",
    "def calculate_accuracy(predict_function, X_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(X_test)\n",
    "    \n",
    "    # Iterate over each sample in the testing data\n",
    "    for i in range(total_samples):\n",
    "        # Use the predict_function to predict sentiment for the current sample\n",
    "        predicted_label = predict_function(X_test[i])\n",
    "        \n",
    "        # Compare the predicted label with the true label\n",
    "        if predicted_label == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you have your predict_sentiment function and testing data ready\n",
    "def predict_sentiment(text):\n",
    "    log_likelihood = 0\n",
    "    log_prior = 0\n",
    "    \n",
    "    for word in text.split():\n",
    "        try:\n",
    "            lamda_value = frequency_df.loc[word, 'lamda']\n",
    "        except KeyError:\n",
    "            continue  # Skip this word if it's not found in frequency_df\n",
    "        else:\n",
    "            if not pd.isna(lamda_value):\n",
    "                log_likelihood += lamda_value\n",
    "    \n",
    "    # Add log prior to the log-likelihood score\n",
    "    log_score = log_likelihood + log_prior\n",
    "    \n",
    "    # Predict sentiment based on log score\n",
    "    if log_score > 0:\n",
    "        return 1\n",
    "    elif log_score < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Calculate accuracy on testing set\n",
    "accuracy = calculate_accuracy(predict_sentiment, balanced_test['stemmed_content'], balanced_test['target']) \n",
    "print(\"Accuracy on testing set:\", accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming (content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)  # remove non alphabetic characters and replace them with\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    \n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    port_stem = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [port_stem.stem(word) for word in words if word not in stop_words]\n",
    "\n",
    "    print(words)\n",
    "    # Join the words back into a string\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smarrison', 'would', 'first', 'gun', 'realli', 'though', 'zac', 'snyder', 'doucheclown']\n"
     ]
    }
   ],
   "source": [
    "text = \"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown\"\n",
    "words_sample_test = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2943784419326936\n"
     ]
    }
   ],
   "source": [
    "loglikelihood =0\n",
    "for i in words_sample_test:\n",
    "    if pd.isna(frequency_df.loc[i, 'lamda']) :\n",
    "        pass \n",
    "    else:\n",
    "        loglikelihood += frequency_df.loc[i, 'lamda']\n",
    "\n",
    "print(loglikelihood)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Compute the log-likelihood score for the tweet\n",
    "    words_sample_test = preprocess_text(text)\n",
    "    log_likelihood =0\n",
    "    log_prior =0\n",
    "    for i in words_sample_test:\n",
    "        if pd.isna(frequency_df.loc[i, 'lamda']) :\n",
    "            pass \n",
    "        else:\n",
    "            log_likelihood += frequency_df.loc[i, 'lamda']\n",
    "    # Add log prior to the log-likelihood score\n",
    "    log_score = log_likelihood + log_prior\n",
    "    \n",
    "    # Predict sentiment based on log score\n",
    "    if log_score > 0:\n",
    "        return 1\n",
    "    elif log_score < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ram', 'go', 'mumbai']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input(\"Enter you statement for sentiment analysis\")\n",
    "predict_sentiment(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
